<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <!-- <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/> -->

  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> -->


  <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by-->
  <!-- <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1"> -->


  <title>SLANG: New Concept Comprehension of Large Language Models</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">SLANG: New Concept Comprehension of Large Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="http://me.meirtz.com/" target="_blank">Lingrui Mei</a><sup>1,3</sup>,
              </span>
              <span class="author-block">
                  <a  target="_blank">Shenghua Liu</a><sup>1,3*</sup>,
              </span>
              <span class="author-block">
                <a target="_blank">Yiwei Wang</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a target="_blank">Baolong Bi</a><sup>1,3</sup>,
              </span>
              <span class="author-block">
                <a target="_blank">Xueqi Cheng</a><sup>1,3</sup>,
              </span>
              <span class="author-block">
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>CAS Key Laboratory of AI Security, Institute of Computing Technology, Chinese Academy of Sciences<br><sup>2</sup>University of California, Los Angeles<br><sup>3</sup>University of Chinese Academy of Sciences</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding author.</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2401.12585.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Meirtz/FocusOnSlang-Toolbox" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2401.12585" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="hero-body">
    <div class="container  is-max-desktop">
      <h2 class="title is-3"></h2>
      <div class="level-set has-text-justified">
     </div>
     <img src="static/images/pipeline.png" class="blend-img-background center-image" style=" display: block; margin-left: auto; margin-right: auto; width: 100%; max-width: 800px; height: auto;">
</div>
   </div>
  </section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The dynamic nature of language, particularly evident in the realm of slang and memes on the Internet, poses serious challenges to the adaptability of large language models (LLMs). Traditionally anchored to static datasets, these models often struggle to keep up with the rapid linguistic evolution characteristic of online communities. This research aims to bridge this gap by enhancing LLMs' comprehension of the evolving new concepts on the Internet, without the high cost of continual retraining. In pursuit of this goal, we introduce <strong>SLANG</strong>, a benchmark designed to autonomously integrate novel data and assess LLMs' ability to comprehend emerging concepts, alongside <strong>FOCUS</strong>, an approach uses causal inference to enhance LLMs to understand new phrases and their colloquial context. Our benchmark and approach involves understanding real-world instances of linguistic shifts, serving as contextual beacons, to form more precise and contextually relevant connections between newly emerging expressions and their meanings. The empirical analysis shows that our causal inference-based approach outperforms the baseline methods in terms of precision and relevance in the comprehension of Internet slang and memes.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<section class="container is-max-desktop">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Comparsion</h2>
      <div class="level-set has-text-justified" style="display: flex; align-items: center; justify-content: center;">
        <!-- Flex容器 -->
        <img src="static/images/example.png" class="blend-img-background center-image" style="flex-shrink: 0; width: 100%; max-width: 300px; height: auto; margin-right: 20px;">
        <!-- 图像设置了flex-shrink: 0来防止它在flex容器中缩小 -->
        <p style="flex-grow: 1;">
          Comparsion of LLMs’ understanding of new phrases using CoT (Wei et al., 2022) and FOCUS methods. The left side demonstrates the limited understanding through the CoT approach, focusing on the literal interpretation. In contrast, the right side using the FOCUS method shows the model’s enhanced capability to grasp metaphors and deeper meanings.
        </p>
        <!-- 文本设置了flex-grow: 1来允许它填充剩余空间 -->
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="hero-body">
    <div class="container  is-max-desktop">
      <h2 class="title is-3">SLANG</h2>
      <div class="level-set has-text-justified">
      <p>
        SLANG is a benchmark designed to enhance Large Language Models' (LLMs) comprehension of evolving internet slang and idiomatic expressions without the need for continual retraining. It leverages data from UrbanDictionary to assess LLMs' ability to adapt to linguistic shifts, utilizing a dataset that includes both factual and counterfactual instances for a comprehensive evaluation. The approach focuses on understanding real-world instances of linguistic changes, aiming to form precise and contextually relevant connections between newly emerging expressions and their meanings. Through empirical analysis, this method has demonstrated superior precision and relevance in understanding internet slang and memes compared to traditional methods.
      </p>

     </div>

   </div>
  </section>


<section class="hero teaser">
  <div class="hero-body">
    <div class="container  is-max-desktop">
      <h2 class="title is-3">FOCUS</h2>
      <div class="level-set has-text-justified">
      <p>
        In our research, we introduce FOCUS, a novel approach leveraging causal inference to significantly enhance large language models' (LLMs) comprehension of evolving internet language, such as slang and memes. By examining causal relationships within linguistic contexts, FOCUS advances LLMs beyond traditional correlation-based learning, enabling a nuanced understanding of language dynamics. This method notably improves adaptability and effectiveness in applications requiring deep linguistic comprehension, as demonstrated by superior precision, adaptability metrics, and empirical performance over existing methodologies.
      </p>
     </div>
     <img src="static/images/SCM.png" class="blend-img-background center-image" style=" display: block; margin-left: auto; margin-right: auto; width: 100%; max-width: 800px; height: auto;">
          <p>SCM analysis in FOCUS methodology. This figure presents the complex process of causal inference in the understanding of new phrases, highlighting how the FOCUS approach systematically analyzes and interprets intricate language patterns, emphasizing the causal links between linguistic elements and their interpretive outcomes
      </p></div>
   </div>
  </section>


<section class="hero teaser">
  <div class="hero-body">
    <div class="container  is-max-desktop">
      <h2 class="title is-3">Experiement</h2>
      <div class="level-set has-text-justified">
        <p>
          We meticulously design and execute a series of experiments to assess the efficacy of our proposed methods in enhancing the comprehension of new concepts and slang by large language models. We employ a diverse dataset, encompassing a wide range of newly coined terms and slang, to evaluate the models' ability to grasp these concepts post-intervention. Our experimental setup involves a baseline comparison, where models' performance pre and post-adjustment is rigorously analyzed. The outcomes highlight significant enhancements in the models' understanding and generation capabilities, showcasing our approach's potential in bridging the gap between evolving language trends and static language model training. Through these experiments, we aim to demonstrate the adaptability and increased relevance of language models in processing and interacting with contemporary linguistic expressions.
        </p>

     </div>
     <img src="static/images/result1.jpg" class="blend-img-background center-image" style=" display: block; margin-left: auto; margin-right: auto; width: 100%; max-width: 800px; height: auto;">
     <img src="static/images/result2.jpg" class="blend-img-background center-image" style=" display: block; margin-left: auto; margin-right: auto; width: 100%; max-width: 800px; height: auto;">
      </div>
   </div>


   <section class="hero teaser">
    <div class="hero-body">
      <div class="container  is-max-desktop">
        <h2 class="title is-3">Conclusion</h2>
        <div class="level-set has-text-justified">
          <p>
            In this work, we have explored the dynamic and evolving nature of internet language, particularly slang and memes, and their impact on the adaptability of large language models (LLMs). Our study introduced a novel benchmark, SLANG, to assess LLMs' proficiency in comprehending emerging linguistic trends. Additionally, we proposed the FOCUS methodology, which utilizes causal inference to enhance understanding of new concepts, going beyond other methods in precision and relevance. This success is underpinned by our rigorous methodologies involving the construction of datasets from UrbanDictionary and the incorporation of both factual and counterfactual instances to provide diverse linguistic contexts. The results from our experiments demonstrate the enhanced capability of LLMs, equipped with our FOCUS method, to adapt to the rapid evolution of online language. This research marks a significant advancement in the field of natural language processing, highlighting the importance of contextual understanding and adaptability in LLMs for effectively navigating the complexities of evolving human communication.
          </p>
  
       </div>
      </div>
     </div>
  </section>





  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{mei2024slang,
        title={SLANG: New Concept Comprehension of Large Language Models}, 
        author={Lingrui Mei and Shenghua Liu and Yiwei Wang and Baolong Bi and Xueqi Cheng},
        year={2024},
        eprint={2401.12585},
        archivePrefix={arXiv},
        primaryClass={cs.CL}
  }</code></pre>
    </div>
  </section>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
